{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute reference embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, dataset_path, curated_idx, few_shot_transforms, model\n",
    "from setup import tqdm, torch, np, os, h5py, sns, plt, tqdm, Trans, Image, chain\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DINOSim import DinoSim_pipeline, diplay_features\n",
    "from napari_dinosim.utils import get_img_processing_f\n",
    "# Create an instance of the pipeline (not just assigning the class)\n",
    "\n",
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ref_embeddings = False\n",
    "\n",
    "data_aug = False\n",
    "k = 10\n",
    "\n",
    "if saved_ref_embeddings == False:\n",
    "\n",
    "    files, labels = zip(*get_fnames()) \n",
    "\n",
    "    if data_aug:    \n",
    "        nb_transformations = len(few_shot_transforms)\n",
    "        \n",
    "        # Preload images and metadata once\n",
    "        good_images = []\n",
    "        transformed_coordinates = []\n",
    "\n",
    "        for idx in curated_idx:\n",
    "            img, coord_x, coord_y = load_image(files[idx])\n",
    "            good_images.append(img.transpose(1,2,0))\n",
    "            transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "        transformed_images = []\n",
    "        for image in good_images:\n",
    "            transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "            transformed_images.extend(transformed)\n",
    "\n",
    "        for j, img in enumerate(transformed_images):\n",
    "            if img.shape != torch.Size([130, 130, 1]):\n",
    "                h, w = img.shape[:2]\n",
    "                h_diff = (130 - h) // 2\n",
    "                w_diff = (130 - w) // 2\n",
    "                padded_img = torch.zeros(130, 130, 1)\n",
    "                padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                transformed_images[j] = padded_img\n",
    "                \n",
    "        batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "        good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "        good_datasets = np.array(good_datasets)\n",
    "        \n",
    "        transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "        good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "    else:\n",
    "\n",
    "        imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "        imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "        batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "        imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "        good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "        \n",
    "        good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "        good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "        good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "    unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "    for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "        \n",
    "        # Pre-compute embeddings\n",
    "        few_shot.pre_compute_embeddings(\n",
    "            dataset,  # Pass numpy array of images\n",
    "            overlap=(0.5, 0.5),\n",
    "            padding=(0, 0),\n",
    "            crop_shape=(518, 518, 1),\n",
    "            verbose=True,\n",
    "            batch_size=10\n",
    "        )\n",
    "        \n",
    "        # Set reference vectors\n",
    "        few_shot.set_reference_vector(coordinates, filter=None)\n",
    "        ref = few_shot.get_refs()\n",
    "        \n",
    "        # Get closest elements - using the correct method name\n",
    "        close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "        k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "        \n",
    "        # Convert to numpy for storing\n",
    "        close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "        \n",
    "        filtered_latent_list.append(close_embedding_np)\n",
    "        filtered_label_list.append(k_labels)\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        few_shot.delete_precomputed_embeddings()\n",
    "        few_shot.delete_references()\n",
    "\n",
    "    mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "    # Stack all embeddings and labels\n",
    "    ref_latents = np.vstack(filtered_latent_list)\n",
    "    ref_labels = np.hstack(filtered_label_list)\n",
    "    \n",
    "    torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "    torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "    torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "\n",
    "else:\n",
    "\n",
    "    mean_ref = torch.load('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/g_mean_ref_518x518') #TODO: For Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute new image's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_index = 650\n",
    "\n",
    "img = np.array(load_image(files[new_image_index])[0])[...,np.newaxis]#.transpose(0,2,3,1)\n",
    "    \n",
    "few_shot.pre_compute_embeddings(\n",
    "        img,  # Pass numpy array of images\n",
    "        overlap=(0.5, 0.5),\n",
    "        padding=(0, 0),\n",
    "        crop_shape=(518, 518, 1),\n",
    "        verbose=True,\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "new_img_embs = few_shot.get_embs().reshape(-1, feat_dim)\n",
    "    \n",
    "new_label = ['new' for _ in range(new_img_embs.shape[0])]\n",
    "    \n",
    "# Stack all embeddings and labels\n",
    "ref_latents = np.vstack(latent_list)\n",
    "ref_labels = np.hstack(label_list)\n",
    "    \n",
    "mean_ref = np.vstack([np.mean(list, axis=0) for list in latent_list])\n",
    "mean_labs = [neurotransmitter for neurotransmitter in neurotransmitters]\n",
    "    \n",
    "latents = np.vstack([ref_latents, new_img_embs])  # Changed from stack to vstack for proper concatenation\n",
    "labs = np.hstack([ref_labels, new_label])    # Changed from stack to hstack for proper concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The actual EM belongs to the class: {labels[new_image_index]}')\n",
    "print(f'We have {len(latent_list[0])} reference points inside each of the {len(neurotransmitters)} classes')\n",
    "print(f'There are {len(mean_ref)} average reference embeddings and {len(ref_latents)} in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference embeddings visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 'giant'\n",
    "distance_matrix = euclidean_distances(mean_ref, mean_ref)\n",
    "plt.figure(figsize=(12,7), dpi=100)\n",
    "sns.heatmap(distance_matrix, xticklabels=neurotransmitters, yticklabels=neurotransmitters, cmap='Reds')\n",
    "plt.title(f'Distance matrix - {model_size} DINOv2 - Data augmentation: {data_aug}')\n",
    "plt.xlabel('Reference embeddings')\n",
    "plt.ylabel('Reference embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The sum of distances is {np.sum(np.triu(distance_matrix, k=0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diplay_features(\n",
    "        np.vstack([ref_latents, np.vstack(mean_ref)]),\n",
    "        np.hstack([ref_labels, np.hstack(mean_labs)]),\n",
    "\n",
    "        include_pca=False,\n",
    "        pca_nb_components=100,\n",
    "        clustering=False,\n",
    "        nb_clusters=6,\n",
    "        nb_neighbor=10,\n",
    "        min_dist=1,\n",
    "        nb_components=2,\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "inertia_list = []\n",
    "for k in range(1,len(neurotransmitters)+1):\n",
    "    kmeans = KMeans(n_clusters=k).fit(ref_latents)\n",
    "    inertia_list.append(kmeans.inertia_)\n",
    "plt.figure(figsize=(12,7),dpi=100)\n",
    "plt.plot([i for i in range(1,len(neurotransmitters)+1)], inertia_list)\n",
    "plt.title(f'Reference Embeddings - KMeans inertia - {model_size} DINOv2 - Data augmentation: {data_aug}')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xlabel('Nb of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diplay_features(\n",
    "        latents,\n",
    "        labs,\n",
    "        include_pca=False,\n",
    "        pca_nb_components=100,\n",
    "        clustering=False,\n",
    "        nb_clusters=6,\n",
    "        nb_neighbor=30,\n",
    "        min_dist=1,\n",
    "        nb_components=2,\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_similarity_matrix = euclidean_distances(mean_ref, new_img_embs)\n",
    "test_similarity_matrix_normalized = (test_similarity_matrix - np.min(test_similarity_matrix)) / (np.max(test_similarity_matrix) - np.min(test_similarity_matrix))\n",
    "plt.figure(figsize=(15,5),dpi=300)\n",
    "sns.heatmap(test_similarity_matrix_normalized, yticklabels=mean_labs, cmap='Reds')\n",
    "plt.title(f'Distance matrix New image - References - {model_size} DINOv2 - Data augmentation: {data_aug}')\n",
    "plt.xlabel('Image patches')\n",
    "plt.ylabel('Reference embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_neurotransmitters = np.zeros((len(neurotransmitters),len(neurotransmitters))) + np.identity(len(neurotransmitters))\n",
    "emb_labels = np.hstack([[labels[i+1]]*240000 for i in range(0, 3600, 600)]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_saved = True\n",
    "\n",
    "if embeddings_saved:\n",
    "        new_embeddings = torch.load('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/g_embs_140_140.pt') #TODO: for Mac\n",
    "        #new_embeddings = torch.load('/home/tomwelch/Cambridge/Datasets/g_embs_140_140.pt') #TODO: for LINUX\n",
    "\n",
    "else:\n",
    "        \n",
    "        images = np.array([load_image(file)[0] for file in tqdm(files, desc='Loading images')]).transpose(0,2,3,1)\n",
    "\n",
    "        few_shot.pre_compute_embeddings(\n",
    "                images,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=60\n",
    "                )\n",
    "        new_embeddings = few_shot.get_embs().reshape(-1, feat_dim)\n",
    "\n",
    "        torch.save(new_embeddings, os.path.join(dataset_path, f'{model_size}_embs_{resize_size}_Aug={data_aug}_k={k}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(reference_embeddings = mean_ref, \n",
    "            test_embeddings = new_embeddings,\n",
    "            metric = euclidean_distances,\n",
    "            model_size = model_size,\n",
    "            distance_threshold = 0.1,\n",
    "            data_aug = True,\n",
    "            Cross_Entropy_Loss = False,\n",
    "            MSE_Loss = False):\n",
    "\n",
    "    score_lists = [[],[],[],[],[],[]]\n",
    "\n",
    "    similarity_matrix = metric(reference_embeddings, test_embeddings)\n",
    "    similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "\n",
    "    similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= distance_threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "    for i, label in tqdm(enumerate(emb_labels)):\n",
    "\n",
    "        column = similarity_matrix_normalized_filtered[:,i]\n",
    "\n",
    "        if sum(column) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if Cross_Entropy_Loss:\n",
    "                pass \n",
    "            elif MSE_Loss:\n",
    "                pass \n",
    "            else: \n",
    "                patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "\n",
    "                output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "\n",
    "                gt_index = neurotransmitters.index(label)\n",
    "                ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "                score = np.sum(output_class*ground_truth)\n",
    "                score_lists[gt_index].append(score)\n",
    "\n",
    "    accuracies = [np.mean(scores)*100 for scores in score_lists]\n",
    "\n",
    "    plt.figure(figsize=(12,7), dpi=300)\n",
    "    plt.bar(neurotransmitters, accuracies)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Mean hard accuracy')\n",
    "    plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "    plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "    plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "    plt.legend()\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0,110])\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lists = [[],[],[],[],[],[]]\n",
    "mean_accuracies_list, included_list = [], []\n",
    "\n",
    "similarity_matrix = euclidean_distances(mean_ref, new_embeddings)\n",
    "similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "\n",
    "for distance_threshold in tqdm(np.arange(0.05, 1.025, 0.025)): \n",
    "\n",
    "    similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= distance_threshold, similarity_matrix_normalized, 0)\n",
    "    \n",
    "    included_list.append(len(np.where(similarity_matrix_normalized_filtered !=0)[0]))\n",
    "\n",
    "    for i, label in enumerate(emb_labels):\n",
    "\n",
    "        column = similarity_matrix_normalized_filtered[:,i]\n",
    "\n",
    "        if sum(column) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "\n",
    "            output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "\n",
    "            gt_index = neurotransmitters.index(label)\n",
    "            ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "            score = np.sum(output_class*ground_truth)\n",
    "            score_lists[gt_index].append(score)\n",
    "\n",
    "    mean_accuracies_list.append(np.mean([np.mean(scores)*100 for scores in score_lists]))\n",
    "    \n",
    "included_list = [inclusion/(400*len(files)) for inclusion in included_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7), dpi=100)\n",
    "plt.plot([i for i in np.arange(0.05, 1.025, 0.025)], mean_accuracies_list)\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "plt.title(f'Mean hard accuracies per distance threshold - {model_size} DINOv2 - 140x140 images - Data augmentation: {data_aug}')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7), dpi=100)\n",
    "plt.plot([i for i in np.arange(0.05, 1.025, 0.025)], included_list)\n",
    "plt.xlabel('Thresholds')\n",
    "plt.ylabel('Percentage included')\n",
    "plt.title(f'Percentage of patches included per distance threshold - {model_size} DINOv2 - 140x140 images - Data augmentation: {data_aug}')\n",
    "ax = plt.gca()\n",
    "#ax.set_ylim([0,110])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt with mutual information and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with bigger model, try with added context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perso_utils import get_fnames\n",
    "import h5py\n",
    "from setup import device, feat_dim, resize_size, model\n",
    "from DinoPsd_utils import get_img_processing_f\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.metrics import normalized_mutual_information\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "few_shot = DinoPsd_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            ) \n",
    "\n",
    "files, _ = zip(*get_fnames()) \n",
    "\n",
    "delta = 4\n",
    "\n",
    "master_mi_list = []\n",
    "for path in tqdm(files):\n",
    "        with h5py.File(path) as f:\n",
    "                pre, _ = f['annotations/locations'][:]/8\n",
    "                _, _, z = pre.astype(int)\n",
    "                slice_volume = f['volumes/raw'][:][:,:,z-delta:z+delta+1][None].transpose(3,1,2,0)\n",
    "                \n",
    "        few_shot.pre_compute_embeddings(\n",
    "                slice_volume,\n",
    "                verbose=False,\n",
    "                batch_size=2*delta+1\n",
    "                )\n",
    "\n",
    "        embeddings = few_shot.get_embeddings(reshape=False)\n",
    "        \n",
    "        reference_slice = embeddings[delta]\n",
    "\n",
    "        mi_list = []\n",
    "        for slice in embeddings:\n",
    "                mi = normalized_mutual_information(reference_slice.cpu(), slice.cpu())\n",
    "                mi_list.append(mi)\n",
    "\n",
    "        master_mi_list.append(mi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = np.stack(master_mi_list)\n",
    "mean_list, std_list = [], []\n",
    "for row in MI.T:\n",
    "    mean_list.append(np.mean(row))\n",
    "    std_list.append(np.std(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7), dpi=100)\n",
    "sns.lineplot(x=range(len(mean_list)), y=mean_list, label=\"Mean MI\")\n",
    "plt.fill_between(range(len(mean_list)), np.array(mean_list) - np.array(std_list), np.array(mean_list) + np.array(std_list), alpha=0.3, label=\"±1 SD\")\n",
    "plt.xlabel('Slice')\n",
    "plt.ylabel('Mutual Information')\n",
    "plt.title('Mutual Information per Slice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
