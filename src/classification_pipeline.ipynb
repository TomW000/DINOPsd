{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0924a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d004fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msetup\u001b[39;00m \u001b[39mimport\u001b[39;00m neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms,  embeddings_path, model\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msetup\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm, torch, np, os, plt, tqdm, gc, sample\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39manalysis_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .setup import neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms,  embeddings_path, model\n",
    "from .setup import tqdm, torch, np, os, plt, tqdm, gc, sample\n",
    "from .analysis_utils import display_hdf_image_grid, resize_hdf_image, get_augmented_coordinates\n",
    "from .setup import cosine_similarity, euclidean_distances\n",
    "from .perso_utils import get_fnames, load_image, get_latents\n",
    "from .DinoPsd import DinoPsd_pipeline\n",
    "from .DinoPsd_utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fba314",
   "metadata": {},
   "source": [
    "### Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoPsd_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_refs = compute_ref_embeddings(True, os.path.join(embeddings_path, 'giant_mean_ref_518_Aug=False_k=10'))\n",
    "mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=True):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "            imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "            imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "            good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "            \n",
    "            good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "            good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "            good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "        unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "        for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "            \n",
    "            # Pre-compute embeddings\n",
    "            few_shot.pre_compute_embeddings(\n",
    "                dataset,  # Pass numpy array of images\n",
    "                overlap=(0.5, 0.5),\n",
    "                padding=(0, 0),\n",
    "                crop_shape=(518, 518, 1),\n",
    "                verbose=True,\n",
    "                batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Set reference vectors\n",
    "            few_shot.set_reference_vector(coordinates, filter=None)\n",
    "            ref = few_shot.get_refs()\n",
    "            \n",
    "            # Get closest elements - using the correct method name\n",
    "            close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "            k_labels =  [batch_label for _ in range(k)]\n",
    "\n",
    "            \n",
    "            # Convert to numpy for storing\n",
    "            close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "            \n",
    "            filtered_latent_list.append(close_embedding_np)\n",
    "            filtered_label_list.append(k_labels)\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            few_shot.delete_precomputed_embeddings()\n",
    "            few_shot.delete_references()\n",
    "\n",
    "        mean_ref = torch.from_numpy(np.vstack([np.mean(l, axis=0) for l in filtered_latent_list]))\n",
    "        # Stack all embeddings and labels\n",
    "        ref_latents = np.vstack(filtered_latent_list)\n",
    "        ref_labels = np.hstack(filtered_label_list)\n",
    "        \n",
    "#        torch.save(mean_ref, os.path.join(dataset_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    " #       torch.save(ref_latents, os.path.join(dataset_path, f'{model_size}_ref_latents_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "  #      torch.save(ref_labels, os.path.join(dataset_path, f'{model_size}_ref_labels_{resize_size}_Aug={data_aug}_k={k}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb7274",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_neurotransmitters = np.eye(len(neurotransmitters))\n",
    "emb_labels = np.hstack([[neuro]*int((resize_size/14)**2 * 600) for neuro in neurotransmitters]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859a151",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = compute_embeddings()\n",
    "new_embeddings = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt')) # takes ~ 45 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef9b0c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute class-wise accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from analysis_utils import get_threshold\n",
    "\n",
    "def compute_accuracies(reference_embeddings = mean_refs, \n",
    "                       embeddings = new_embeddings,\n",
    "                       metric = euclidean_distances,\n",
    "                       distance_threshold = 0.01\n",
    "                       ):\n",
    "\n",
    "    batch_size = int(len(embeddings)/6)\n",
    "\n",
    "    for n, i in tqdm(enumerate(range(0, len(embeddings), batch_size))):\n",
    "        batch = embeddings[i:i+batch_size]\n",
    "\n",
    "        #embeddings = embeddings.reshape(-1, feat_dim)\n",
    "        similarity_matrix = metric(reference_embeddings, batch)\n",
    "        similarity_matrix_normalized = (similarity_matrix - np.min(similarity_matrix)) / (np.max(similarity_matrix) - np.min(similarity_matrix))\n",
    "        threshold = get_threshold(similarity_matrix_normalized, 0.9)\n",
    "        similarity_matrix_normalized_filtered = np.where(similarity_matrix_normalized <= threshold, similarity_matrix_normalized, 0)\n",
    "\n",
    "        batch_score_list = []\n",
    "        for k in range(batch_size):\n",
    "\n",
    "            column = similarity_matrix_normalized_filtered[:,k]\n",
    "            j=0\n",
    "            if sum(column) == 0:\n",
    "                j+=1\n",
    "            else:\n",
    "                patch_wise_distances_filtered = np.where(column == 0, 1, column)\n",
    "                output_class = one_hot_neurotransmitters[np.argmin(patch_wise_distances_filtered)]\n",
    "                gt_index = n\n",
    "                ground_truth = one_hot_neurotransmitters[gt_index]\n",
    "                score = np.sum(output_class*ground_truth)\n",
    "                batch_score_list.append(score)\n",
    "                \n",
    "        yield batch_score_list\n",
    "\n",
    "g = compute_accuracies()\n",
    "score_list = []\n",
    "\n",
    "for _ in range(6): score_list.append(next(g))\n",
    "\n",
    "accuracies = [np.mean(scores)*100 for scores in score_list]\n",
    "#print(f'{j} embeddings did not pass the threshold')\n",
    "#return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6871dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,7), dpi=300)\n",
    "plt.bar(neurotransmitters, accuracies)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Mean hard accuracy')\n",
    "#plt.title(f'Mean hard accuracies across classes - {model_size} DINOv2 - 140x140 images - Threshold = {distance_threshold} - Data augmentation: {data_aug}')\n",
    "plt.axhline(np.mean(accuracies), color='r', linestyle='--', label='Average')\n",
    "plt.axhline(y=(100/6), color='b', linestyle='--', label='Randomness')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,110])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524483d",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7510ba",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning.neuro_classification.Neuro_Classification_Head import head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e494ff",
   "metadata": {},
   "source": [
    "# Plot MLP Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daee14ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchview\u001b[39;00m \u001b[39mimport\u001b[39;00m draw_graph\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_graph \u001b[39m=\u001b[39m draw_graph(head, input_size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,feat_dim), expand_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_graph\u001b[39m.\u001b[39mvisual_graph\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_dim' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(head, input_size=(1,feat_dim), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c93a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c02e3bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tomwelch/Cambridge/Code/DINOPsd/src/classification_pipeline.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfine_tuning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneuro_classification\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mNeuro_Classification_training\u001b[39;00m \u001b[39mimport\u001b[39;00m head_training\n",
      "File \u001b[0;32m~/Cambridge/Code/DINOPsd/src/fine_tuning/neuro_classification/Neuro_Classification_training.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mNeuro_Classification_Head\u001b[39;00m \u001b[39mimport\u001b[39;00m head\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mNeuro_Classification_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m training_loader, test_loader\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msetup\u001b[39;00m \u001b[39mimport\u001b[39;00m device\n\u001b[1;32m     10\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(head\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m3e-4\u001b[39m)\n",
      "File \u001b[0;32m~/Cambridge/Code/DINOPsd/src/fine_tuning/neuro_classification/Neuro_Classification_dataset.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDinoPsd\u001b[39;00m \u001b[39mimport\u001b[39;00m DinoPsd_pipeline\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDinoPsd_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_img_processing_f\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mFine_Tuning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompute_embeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_embeddings\n",
      "File \u001b[0;32m~/Cambridge/Code/DINOPsd/src/DinoPsd.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msetup\u001b[39;00m \u001b[39mimport\u001b[39;00m umap, torch, tqdm, np, px, PCA, KMeans, h5py, os, ceil, floor\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msetup\u001b[39;00m \u001b[39mimport\u001b[39;00m device, model, feat_dim, resize_size, dataset_path, neurotransmitters\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mDinoPsd_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m resizeLongestSide, mirror_border, crop_data_with_overlap\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from fine_tuning.neuro_classification.Neuro_Classification_training import head_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1\n",
    "loss_list, proportion_list, prediction_list, test_accuracies = head_training(nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3785c58",
   "metadata": {},
   "source": [
    "# Class proportions during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa85bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import class_proprtions\n",
    "\n",
    "class_proprtions(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225436ad",
   "metadata": {},
   "source": [
    "# Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import confusion_matrix\n",
    "\n",
    "split = '80/20'\n",
    "confusion_matrix(prediction_list, nb_epochs, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f4da8",
   "metadata": {},
   "source": [
    "# Progression curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tuning.display_results import training_curve\n",
    "\n",
    "training_curve(epochs, loss_list, test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836307",
   "metadata": {},
   "source": [
    "============================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219d0d2",
   "metadata": {},
   "source": [
    "# -UMAP Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae376e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_file = get_fnames()[1][0]\n",
    "ex_image = resize_hdf_image(load_image(ex_file)[0])[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30714c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_augmented_few_shot = DinoPsd_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b584bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(list(EX_EMBEDDING.values())[0]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb280e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "untrained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "EX_EMBEDDING = augmented_model[0].forward_features(torch.from_numpy(np.concatenate([ex_image, ex_image, ex_image], axis=3).transpose(0,3,1,2)).to(torch.float32).to(device))#\n",
    "\n",
    "ex = untrained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "neuro = 1\n",
    "EX_REF = torch.tensor(REFS[neuro]).cpu()\n",
    "\n",
    "embeddings_and_ref = np.vstack([EX_REF, EX_EMBEDDING])\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "semantic_distances = compute_similarity_matrix(EX_REF, EX_EMBEDDING)\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'Before training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'Before training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd87a66",
   "metadata": {},
   "source": [
    "# -Training augmented model with frozen MLP Head (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac75c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454a44dd",
   "metadata": {},
   "source": [
    "# -UMAP After:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004ed27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_augmented_few_shot = DinoSim_pipeline(augmented_model[0],\n",
    "                                      augmented_model[0].patch_size,\n",
    "                                      device,\n",
    "                                      get_img_processing_f(resize_size),\n",
    "                                      feat_dim, \n",
    "                                      dino_image_size=resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFS = compute_ref_embeddings(True, os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'))\n",
    "\n",
    "trained_augmented_few_shot.pre_compute_embeddings(\n",
    "    ex_image,\n",
    "    verbose=True,\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "NEW_EX_EMBEDDING = trained_augmented_few_shot.get_embeddings(reshape=True).cpu()\n",
    "\n",
    "EX_REF = torch.tensor(REFS[0]).cpu()\n",
    "\n",
    "new_embeddings_and_ref = np.vstack([EX_REF, NEW_EX_EMBEDDING])\n",
    "\n",
    "\n",
    "\n",
    "N = nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "ref_coords = list(map(resize, coords[0][0]))\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "\n",
    "\n",
    "new_umap_embeddings = reducer.fit_transform(new_embeddings_and_ref)\n",
    "\n",
    "from analysis_utils import compute_similarity_matrix\n",
    "\n",
    "new_semantic_distances = compute_similarity_matrix(EX_REF, NEW_EX_EMBEDDING)\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(20,10), dpi=100)\n",
    "\n",
    "sc = ax1.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=new_semantic_distances.ravel(), s=2, cmap='bwr')\n",
    "ax1.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Reference embedding')\n",
    "cbar1 = plt.colorbar(sc, ax=ax1)\n",
    "cbar1.set_label('Semantic distance')\n",
    "ax1.legend()\n",
    "ax1.set_title(f'After training - Semantic distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "sc2 = ax2.scatter(new_umap_embeddings[1:,0], new_umap_embeddings[1:,1], c=distances, s=2, cmap='Greens_r')\n",
    "ax2.scatter(new_umap_embeddings[0,0], new_umap_embeddings[0,1], c='lime', marker='o', label='Mean acc reference embedding')\n",
    "cbar2 = plt.colorbar(sc2, ax=ax2)\n",
    "cbar2.set_label('Spatial distance')\n",
    "ax2.legend()\n",
    "ax2.set_title(f'After training - Spatial distances to {neurotransmitters[neuro]} mean reference embedding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f7061",
   "metadata": {},
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = list(range(1, nb_iterations+1))\n",
    "fig, ax1 = plt.subplots(figsize=(7, 5), dpi=150)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Training\n",
    "lns1 = ax1.plot(x, general_loss_list[0], label='DINOv2 Train Loss', color='blue')\n",
    "\n",
    "lns2 = ax1.plot(x, general_loss_list[1], label='MLP Train Loss', color='cyan')\n",
    "ax1.set_ylim(0, max(max(general_loss_list)) * 1.05)\n",
    "ax1.set_ylabel('Train Loss')\n",
    "\n",
    "#Test\n",
    "lns3 = ax2.plot(x, general_accuracy_list[0], label='DINOv2 Test Accuracy', color='green')\n",
    "\n",
    "\n",
    "lns4 = ax2.plot(x, general_accuracy_list[1], label='MLP Test Accuracy', color='lime')\n",
    "ax2.set_ylim(0, 100 * 1.05)\n",
    "ax2.set_ylabel('Test Accuracy')\n",
    "\n",
    "\n",
    "# Combine legends\n",
    "lns = lns1 + lns2 + lns3 + lns4\n",
    "labels = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labels, loc=6)\n",
    "\n",
    "plt.locator_params(axis='x', nbins=2)\n",
    "\n",
    "ax1.set_xlabel('Iterations')\n",
    "plt.title('Training Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d466672",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85355df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from setup import embeddings_path, feat_dim\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "\n",
    "print('Loading embeddings')\n",
    "\n",
    "EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))[:600]\n",
    "EMBEDDINGS = torch.cat(EMBEDDINGS).reshape(-1, feat_dim)\n",
    "\n",
    "print('Done loading embeddings')\n",
    "print('Computing UMAP')\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(EMBEDDINGS)\n",
    "\n",
    "plt.plot(umap_embeddings[:,0], umap_embeddings[:,1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .setup import feat_dim\n",
    "print(feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce674d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "embeddings_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Embeddings'\n",
    "feat_dim = 384\n",
    "\n",
    "REFS = torch.load(os.path.join(embeddings_path, 'small_mean_ref_518_Aug=False_k=10.pt'), weights_only=False)\n",
    "\n",
    "_EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "EMBEDDINGS=[]\n",
    "for e in _EMBEDDINGS:\n",
    "    EMBEDDINGS.extend(e.reshape(-1, feat_dim))\n",
    "\n",
    "similarity_matrix = euclidean_distances(np.mean(REFS, axis=0)[None], EMBEDDINGS)\n",
    "\n",
    "#l=[]\n",
    "#for i in range(similarity_matrix.shape[1]):\n",
    "#    l.append(np.mean(similarity_matrix[:,i]))\n",
    "\n",
    "flattened_matrix = similarity_matrix.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(flattened_matrix, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eadf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = Psd_Pred_MLP_Head(device=device, feat_dim=feat_dim)\n",
    "classification_head.load_state_dict(torch.load(os.path.join(model_weights_path, 'psd_head_weights.pt')))\n",
    "classification_head.eval()\n",
    "\n",
    "print('Loading embeddings...')\n",
    "\n",
    "IMAGE_EMBEDDINGS = torch.load(os.path.join(embeddings_path, 'small_dataset_embs_518.pt'))\n",
    "IMAGE_EMBEDDINGS = torch.tensor(np.vstack(IMAGE_EMBEDDINGS))[:600]\n",
    "\n",
    "print('...done loading embeddings (check memory usage)')\n",
    "\n",
    "image_size = IMAGE_EMBEDDINGS.shape[1]\n",
    "\n",
    "file_names, _ = zip(*get_fnames())\n",
    "\n",
    "DATA = list(zip(file_names, IMAGE_EMBEDDINGS))\n",
    "\n",
    "def segmented_image_generator():\n",
    "    for file_name, image in tqdm(DATA, total=len(DATA), desc='Segmenting', leave=False):\n",
    "        flattened_image = image.reshape(-1, feat_dim)\n",
    "        flattened_image = flattened_image.to(device)\n",
    "        image_prediction = []\n",
    "        for patch in tqdm(flattened_image, desc='Predicting', leave=False):\n",
    "            with torch.no_grad():\n",
    "                prediction = classification_head(patch)\n",
    "                print(prediction)\n",
    "                prediction = torch.round(prediction).cpu().numpy()\n",
    "                image_prediction.append(prediction)\n",
    "\n",
    "        mask = np.array(image_prediction).reshape(image_size, image_size)\n",
    "        \n",
    "        yield mask\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        h, w = mask.shape\n",
    "        fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "        initial_image = load_image(file_name)[0]\n",
    "        resized_image = resize_hdf_image(initial_image)\n",
    "\n",
    "        ax.imshow(resized_image, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "        # Overlay the heatmap\n",
    "        sns.heatmap(\n",
    "            mask,\n",
    "            cmap='coolwarm',\n",
    "            alpha=0.5,             # Make the heatmap semi-transparent\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False\n",
    "        )\n",
    "\n",
    "        plt.title(\"Segmentation\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        '''\n",
    "        \n",
    "mask_generator = segmented_image_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf025c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    l = []\n",
    "    #main()\n",
    "\n",
    "    for mask in tqdm(mask_generator, desc='Looping through images', leave=False):\n",
    "        l.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1db32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a4316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: For LINUX:\n",
    "dataset_path = '/home/tomwelch/Cambridge/Datasets/neurotransmitter_data'\n",
    "\n",
    "#TODO: For MAC:\n",
    "#dataset_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/neurotransmitter_data'\n",
    "\n",
    "\n",
    "#TODO: For LINUX:\n",
    "embeddings_path = '/home/tomwelch/Cambridge/Embeddings'\n",
    "\n",
    "#TODO: For MAC:\n",
    "#embeddings_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Embeddings'\n",
    "\n",
    "save_path ='/home/tomwelch/Cambridge/Output' \n",
    "\n",
    "#TODO: For LINUX:\n",
    "model_weights_path = '/home/tomwelch/Cambridge/model_weights'\n",
    "\n",
    "#TODO: For MAC:\n",
    "#model_weights_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/model_weights'\n",
    "\n",
    "from glob import glob\n",
    "import h5py\n",
    "import torchvision\n",
    "import torchvision.transforms as Trans\n",
    "import os \n",
    "import torch\n",
    "\n",
    "dates = sorted(glob(os.path.join(dataset_path, '*')))\n",
    "neurotransmitters = sorted(list(map(lambda x: os.path.basename(os.path.normpath(x)), glob(os.path.join(dates[0], '*'))))) #@param {type:\"string\"} \n",
    "\n",
    "\n",
    "def get_fnames():\n",
    "    files_list, labels_list = [], []\n",
    "    for date in dates:\n",
    "        for neuro in neurotransmitters:\n",
    "            fnames = glob(os.path.join(date, neuro, '*.hdf*'))\n",
    "            files_list.append(fnames)\n",
    "            labels_list.append([neuro for _ in range(len(fnames))])\n",
    "    files = sorted(np.concatenate(files_list), key=os.path.basename)\n",
    "    labels = sorted(np.concatenate(labels_list), key=os.path.basename)\n",
    "    return list(zip(files, labels))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_image(path):\n",
    "    with h5py.File(path) as f:\n",
    "        pre, _ = f['annotations/locations'][:]/8\n",
    "        x, y, z = pre.astype(int)\n",
    "        slice_volume = f['volumes/raw'][:][np.newaxis,:,:,z]\n",
    "        return slice_volume, x, y\n",
    "    \n",
    "resize_size = 256\n",
    "\n",
    "def resize_hdf_image(img):\n",
    "    image = torch.from_numpy(img)\n",
    "    resized_image = Trans.Resize(size=resize_size, interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True)(image).permute(1,2,0).numpy()\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3e2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from skimage import measure\n",
    "\n",
    "dataset_path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test'\n",
    "\n",
    "targets = sorted(os.listdir(os.path.join(dataset_path, 'x')), key=os.path.basename)[1:]\n",
    "gt = sorted(os.listdir(os.path.join(dataset_path, 'y')), key=os.path.basename)[1:]\n",
    "\n",
    "data = list(zip(targets, gt))\n",
    "\n",
    "def get_largest_gt(target, gt):\n",
    "    l = []\n",
    "    for i, slice in enumerate(gt):\n",
    "        labelled = measure.label(slice)\n",
    "        rp = measure.regionprops(labelled)\n",
    "        for region in rp:\n",
    "            l.append([i, region.area])\n",
    "\n",
    "    best_idx = max(l, key=lambda x: x[1])[0]\n",
    "\n",
    "    return target[best_idx], gt[best_idx]\n",
    "\n",
    "for target, gt in data:\n",
    "    from tifffile import imread\n",
    "    t = torch.from_numpy(imread(os.path.join(dataset_path, 'x', target)))\n",
    "    from tifffile import imread\n",
    "    g = torch.from_numpy(imread(os.path.join(dataset_path, 'y', gt)))\n",
    "    t = Trans.Resize(size=(resize_size, resize_size), interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True)(t)\n",
    "    g = Trans.Resize(size=(resize_size, resize_size), interpolation=torchvision.transforms.InterpolationMode.BICUBIC, antialias=True)(g)\n",
    "    target, gt = get_largest_gt(t, g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
