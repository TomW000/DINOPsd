{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "from analysis_utils import get_threshold, display_image_grid, resize_image, compute_similarity_matrix\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, dataset_path, curated_idx, few_shot_transforms, model\n",
    "from setup import tqdm, torch, np, os, h5py, sns, plt, tqdm, Trans\n",
    "from perso_utils import get_fnames, load_image\n",
    "from DINOSim import DinoSim_pipeline\n",
    "from napari_dinosim.utils import get_img_processing_f\n",
    "import torchvision\n",
    "# Create an instance of the pipeline (not just assigning the class)\n",
    "\n",
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f7421",
   "metadata": {},
   "source": [
    "# Embedding space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = imread('/Users/tomw/Downloads/frarobo_conn_18924918_images.tif')\n",
    "plt.imshow(o[4], cmap='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "import matplotlib.pyplot as plt\n",
    "numpy_data = imread('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test/y/frarobo_conn_18924918_masks.tif')\n",
    "numpy_data.shape\n",
    "\n",
    "plt.imshow(numpy_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ref_embeddings = False # (but has to be done again)\n",
    "\n",
    "data_aug = False\n",
    "\n",
    "if saved_ref_embeddings == False:\n",
    "\n",
    "    files, labels = zip(*get_fnames()) \n",
    "\n",
    "    if data_aug:    \n",
    "        nb_transformations = len(few_shot_transforms)\n",
    "        \n",
    "        # Preload images and metadata once\n",
    "        good_images = []\n",
    "        transformed_coordinates = []\n",
    "\n",
    "        for idx in curated_idx:\n",
    "            img, coord_x, coord_y = load_image(files[idx])\n",
    "            good_images.append(img.transpose(1,2,0))\n",
    "            transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "        transformed_images = []\n",
    "        for image in good_images:\n",
    "            transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "            transformed_images.extend(transformed)\n",
    "\n",
    "        for j, img in enumerate(transformed_images):\n",
    "            if img.shape != torch.Size([130, 130, 1]):\n",
    "                h, w = img.shape[:2]\n",
    "                h_diff = (130 - h) // 2\n",
    "                w_diff = (130 - w) // 2\n",
    "                padded_img = torch.zeros(130, 130, 1)\n",
    "                padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                transformed_images[j] = padded_img\n",
    "                \n",
    "        batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "        good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "        good_datasets = np.array(good_datasets)\n",
    "        \n",
    "        transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "        good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "    else:\n",
    "\n",
    "        imgs_coords = [load_image(files[idx]) for idx in curated_idx]\n",
    "        imgs, xs, ys = zip(*imgs_coords)\n",
    "\n",
    "        batch_size = int(len(curated_idx)/len(neurotransmitters))\n",
    "        imgs = [imgs[i:i+batch_size] for i in range(0,len(imgs),batch_size)]\n",
    "        good_datasets = np.array(imgs).transpose(0,1,3,4,2)\n",
    "        \n",
    "        good_coordinates = [(0, x, y) for x, y in zip(xs, ys)]\n",
    "        good_coordinates = [good_coordinates[i:i+batch_size] for i in range(0,len(good_coordinates),batch_size)]\n",
    "        good_coordinates = np.array(good_coordinates)\n",
    "\n",
    "\n",
    "    unfiltered_ref_latents_list, filtered_latent_list, filtered_label_list = [], [], []\n",
    "    for dataset, batch_label, coordinates in tqdm(zip(good_datasets, neurotransmitters, good_coordinates), desc='Iterating through neurotransmitters'):\n",
    "        \n",
    "        # Pre-compute embeddings\n",
    "        few_shot.pre_compute_embeddings(\n",
    "            dataset,  # Pass numpy array of images\n",
    "            overlap= (0, 0), #(0.5, 0.5),\n",
    "            padding=(0, 0),\n",
    "            crop_shape=(518, 518, 1),\n",
    "            verbose=True,\n",
    "            batch_size=10\n",
    "        )\n",
    "        \n",
    "        unfiltered_ref_latents = few_shot.get_embs()\n",
    "        \n",
    "        unfiltered_ref_latents_list.append(unfiltered_ref_latents)\n",
    "\n",
    "        # Clean up to free memory\n",
    "        few_shot.delete_precomputed_embeddings()\n",
    "        few_shot.delete_references()\n",
    "\n",
    "    unfiltered_ref_latents = np.array(unfiltered_ref_latents_list)\n",
    "    \n",
    "    #torch.save(unfiltered_ref_latents, os.path.join('/home/tomwelch/Cambridge/Embeddings', f'Unfiltered_ref_latents_model_size={model_size}_img_size={resize_size}_Aug={data_aug}.pt')) #TODO: For Linux\n",
    "    torch.save(unfiltered_ref_latents, os.path.join('/Users/tomw/Documents/MVA/Internship/Cambridge/Embeddings', f'Unfiltered_ref_latents_model_size={model_size}_img_size={resize_size}_Aug={data_aug}.pt')) #TODO: For Mac\n",
    "else:\n",
    "\n",
    "    unfiltered_ref_latents = torch.load('/home/tomwelch/Cambridge/Embeddings/Unfiltered_ref_latents_model_size=giant_img_size=518_Aug=False.pt', weights_only=False) #TODO: For Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ae00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_aug:\n",
    "    pass\n",
    "else:\n",
    "    nb_transformations = 1\n",
    "    \n",
    "nb_images = (len(curated_idx)*nb_transformations)\n",
    "nb_patches_per_image = int(unfiltered_ref_latents.shape[1]*(len(neurotransmitters)/len(curated_idx)))\n",
    "nb_images_per_class = int((len(curated_idx)*nb_transformations)/len(neurotransmitters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for neuro in unfiltered_ref_latents:\n",
    "    ls.append([neuro[i:i+nb_patches_per_image] for i in range(0, unfiltered_ref_latents.shape[1], nb_patches_per_image)])\n",
    "embds = np.array(ls)\n",
    "embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embs = list \n",
    "\n",
    "#i,j wassersetin distance between each embedding sets 60x60 matrix\n",
    "\n",
    "# Wassersetin might be symetric but f-Divergents are not\n",
    "\n",
    "ref_embeddings = unfiltered_ref_latents.reshape(60, 1369, 1536)\n",
    "\n",
    "A_dim = ref_embeddings.shape[0]\n",
    "\n",
    "A = np.zeros((A_dim, A_dim))\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "flattened = [e.ravel() for e in ref_embeddings]\n",
    "\n",
    "def compute_pair(i, j):\n",
    "    dist = wasserstein_distance(flattened[i], flattened[j])\n",
    "    return (i, j, dist)\n",
    "\n",
    "pairs = [(i, j) for i in range(A_dim) for j in range(i, A_dim)]\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_pair)(i, j) for i, j in tqdm(pairs))\n",
    "\n",
    "for i, j, dist in results:\n",
    "    A[i, j] = A[j, i] = dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "labels = list(itertools.chain.from_iterable([[neuro]*nb_images_per_class for neuro in neurotransmitters]))\n",
    "colormap = dict(zip(neurotransmitters, ['red', 'blue', 'green', 'pink', 'yellow', 'grey']))\n",
    "\n",
    "colors = [colormap[label] for label in labels]\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "plt.figure(figsize=(12,7), dpi = 300)\n",
    "G = nx.Graph()\n",
    "for i in range(A_dim):\n",
    "    for j in range(A_dim):\n",
    "        G.add_edge(i, j, weight = np.exp(1000*A[i,j]))\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "nx.draw(G, node_color = colors, pos=nx.spring_layout(G))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6827128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "inertia_list = []\n",
    "n_tests = 500 \n",
    "for k in tqdm(range(1,n_tests)):\n",
    "    kmeans = KMeans(n_clusters=k).fit(embds[0][0])\n",
    "    inertia_list.append(kmeans.inertia_)\n",
    "plt.figure(figsize=(12,7),dpi=100)\n",
    "plt.plot([i for i in range(1, n_tests)], inertia_list)\n",
    "plt.title(f'Reference Embeddings - KMeans inertia - {model_size} DINOv2 - Data augmentation: {data_aug}')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xlabel('Nb of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9ef0e",
   "metadata": {},
   "source": [
    "# Context Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files, labels = zip(*get_fnames()) \n",
    "\n",
    "few_shot.pre_compute_embeddings(\n",
    "np.array(load_image(files[0])[0])[...,np.newaxis],  # Pass numpy array of images\n",
    "overlap= (0, 0), #(0.5, 0.5),\n",
    "padding=(0, 0),\n",
    "crop_shape=(518, 518, 1),\n",
    "verbose=True,\n",
    "batch_size=1\n",
    ")\n",
    "\n",
    "latents = np.array(few_shot.get_embeddings()) # should be of shape (nb_slices, nb_patches, embedding_dim)\n",
    "nb_patches = int((resize_size/14)**2)\n",
    "original = latents.reshape(nb_patches, feat_dim)\n",
    "\n",
    "few_shot.delete_precomputed_embeddings()\n",
    "few_shot.delete_references()\n",
    "\n",
    "augmented_list = []\n",
    "\n",
    "for delta in range(1,11,2):\n",
    "    augmented_list.append(add_context_to_embeddings(files[0], delta)[0])\n",
    "    \n",
    "augmented_list.insert(0,original)\n",
    "\n",
    "deltas = [i for i in range(1,11,2)]\n",
    "deltas.insert(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b477a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_wise_similarities = [np.linalg.norm(v - original, ord = 'fro') for v in augmented_list]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "patch_wise_similarities_list = []\n",
    "\n",
    "for n in range(nb_patches):\n",
    "    patch_wise_similarities_list.append([np.dot(original[n], v[n])/(np.linalg.norm(original[n])*np.linalg.norm(v[n])) for v in augmented_list])\n",
    "\n",
    "#l = np.array([cosine_similarity(original, v) for v in augmented_list])\n",
    "#lll = np.mean(l)\n",
    "\n",
    "patch_wise_similarities = np.mean(np.array(patch_wise_similarities_list), axis = 0)\n",
    "\n",
    "plt.figure(figsize=(12,7), dpi=200)\n",
    "plt.plot(deltas, matrix_wise_similarities)\n",
    "plt.xticks(deltas)\n",
    "plt.title(f'Distance between initial embedding and context-enriched embedding')\n",
    "plt.ylabel('Frobenius distance')\n",
    "plt.xlabel('Delta')\n",
    "#ax = plt.gca()\n",
    "#ax.set_ylim([0,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66596eb",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ad32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files, labels = zip(*get_fnames()) \n",
    "image = load_image(files[0])[0]\n",
    "\n",
    "resized_image = resize_image(image)\n",
    "display_image_grid(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0835fc",
   "metadata": {},
   "source": [
    "## Adding more reference points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ed6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "possibilities = list(product((14,-14, 0),repeat=2))\n",
    "\n",
    "ref_coords = (273,245)\n",
    "augmentations = [np.add(ref_coords,possibility) for possibility in possibilities]\n",
    "augmented_ref_coordinates = [(0, a[0], a[1]) for a in augmentations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "plt.imshow(resized_image, cmap='grey')\n",
    "plt.xticks([i for i in range(0,518,14)])\n",
    "plt.yticks([i for i in range(0,518,14)])\n",
    "plt.grid(True, color='r', linewidth=1)\n",
    "plt.xticks(rotation = -90)\n",
    "x,y=zip(*augmentations)\n",
    "plt.scatter(x,y, marker='x', c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot.pre_compute_embeddings(\n",
    "    dataset=resized_image[None,:,:,:], \n",
    "    batch_size=1\n",
    "    )\n",
    "embeddings = few_shot.get_embeddings(reshape=False)\n",
    "\n",
    "few_shot.set_reference_vector([(0,273,245)])\n",
    "ref = few_shot.get_reference_embedding()\n",
    "\n",
    "patch_similarities_1 = compute_similarity_matrix(ref, embeddings)\n",
    "\n",
    "h, w = patch_similarities_1.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "# Show the background image\n",
    "ax.imshow(resized_image, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "# Overlay the heatmap\n",
    "sns.heatmap(\n",
    "    patch_similarities_1,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.5,             # Make the heatmap semi-transparent\n",
    "    ax=ax,\n",
    "    cbar=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "\n",
    "plt.title(\"Patch Similarities\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911115c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d363558",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot.pre_compute_embeddings(\n",
    "    dataset=resized_image[None,:,:,:], \n",
    "    batch_size=1\n",
    "    )\n",
    "embeddings = few_shot.get_embeddings(reshape=False)\n",
    "\n",
    "#few_shot.set_reference_vector([(0,273,245)])\n",
    "few_shot.set_reference_vector(augmented_ref_coordinates)\n",
    "ref = few_shot.get_reference_embedding()\n",
    "\n",
    "patch_similarities_2 = compute_similarity_matrix(ref, embeddings)\n",
    "\n",
    "h, w = patch_similarities_2.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "# Show the background image\n",
    "ax.imshow(resized_image, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "# Overlay the heatmap\n",
    "sns.heatmap(\n",
    "    patch_similarities_2,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.5,             # Make the heatmap semi-transparent\n",
    "    ax=ax,\n",
    "    cbar=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "\n",
    "plt.title(\"Patch Similarities with augmented reference coordinates\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = get_threshold(patch_similarities_2, 0.95)\n",
    "pred = patch_similarities_2>threshold\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce47c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.zeros((37,37))\n",
    "gt[10:25,10:25] = np.ones((25-10, 25-10))\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "score = jaccard_score(gt, pred, average='micro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79caa31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = load_image(files[3])[0]\n",
    "resized_new_image = resize_image(new_image)\n",
    "new_dataset = np.stack([image, new_image]).transpose(0,2,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot.pre_compute_embeddings(\n",
    "    dataset=new_dataset, \n",
    "    batch_size=len(new_dataset)\n",
    "    )\n",
    "new_embeddings = few_shot.get_embeddings(reshape=False)\n",
    "\n",
    "patch_similarities_3 = compute_similarity_matrix(ref, new_embeddings[1])\n",
    "\n",
    "h, w = patch_similarities_3.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "# Show the background image\n",
    "ax.imshow(resized_new_image, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "# Overlay the heatmap\n",
    "sns.heatmap(\n",
    "    patch_similarities_3,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.5,             # Make the heatmap semi-transparent\n",
    "    ax=ax,\n",
    "    cbar=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "\n",
    "plt.title(\"Patch Similarities\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized_new_image, cmap='gray', extent=[0, h, w, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc976c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = get_threshold(patch_similarities_3, 0.9)\n",
    "pred = patch_similarities_3 > threshold\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "delta = 2\n",
    "\n",
    "context_patches, augmented_ref = add_context_to_embeddings(files[0], delta=delta)\n",
    "\n",
    "nb_patches_per_dim = int((resize_size/14))\n",
    "\n",
    "similarity_list_4 = []\n",
    "for patch in context_patches:\n",
    "    similarity_list_4.append(np.dot(augmented_ref, patch)/(np.linalg.norm(augmented_ref)*np.linalg.norm(patch)))\n",
    "\n",
    "patch_similarities = np.array(similarity_list_4).reshape(nb_patches_per_dim, nb_patches_per_dim)\n",
    "\n",
    "h, w = patch_similarities.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "# Show the background image\n",
    "ax.imshow(resized_image, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "# Overlay the heatmap\n",
    "sns.heatmap(\n",
    "    patch_similarities,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    "    cbar=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "\n",
    "plt.title(f\"Augmented Patch Similarities - delta={delta}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db063dd",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# UMAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e758e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embeddings_and_ref = np.vstack([ref, new_embeddings])\n",
    "\n",
    "N = nb_patches_per_dim\n",
    "\n",
    "center = (ref_coords[1]//14+1,ref_coords[0]//14+1)\n",
    "row, col = np.ogrid[:N, :N]\n",
    "\n",
    "distance_matrix = np.abs(N - np.maximum(np.abs(row - center[0]), np.abs(col - center[1])) - nb_patches_per_dim)\n",
    "\n",
    "distances = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        distances.append(distance_matrix[i,j])\n",
    "\n",
    "umap_embeddings = reducer.fit_transform(embeddings_and_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5350b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_embeddings[1:,0], umap_embeddings[1:,1], c=similarity_list_2, s=2, cmap='Reds')\n",
    "plt.scatter(umap_embeddings[0,0], umap_embeddings[0,1], c='red', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddable_image(data):\n",
    "    img_data = 255 - 15 * data.astype(np.uint8)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((256, 256), Image.Resampling.LANCZOS)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png', optimize=True)\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8db52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Inferno256  \n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ebfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_blocks = resized_image.reshape(37, 14, 37, 14)\n",
    "patch_blocks_list = []\n",
    "coords_list = []\n",
    "for i in range(nb_patches_per_dim):\n",
    "    for j in range(nb_patches_per_dim):\n",
    "        patch_blocks_list.append(patch_blocks[i,:,j,:])\n",
    "        coords_list.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(umap_embeddings, columns=('x', 'y'))\n",
    "\n",
    "coordinates = [coord for coord in coords_list]\n",
    "coordinates = coordinates[::-1]\n",
    "coordinates.append((ref_coords[1]//14+1,ref_coords[0]//14+1))\n",
    "coordinates = coordinates[::-1]\n",
    "digits_df['Position'] = coordinates\n",
    "\n",
    "patch_blocks_list = patch_blocks_list[::-1]\n",
    "patch_blocks_list.append(patch_blocks[ref_coords[1]//14+1,:,ref_coords[0]//14+1,:])\n",
    "patch_blocks_list = patch_blocks_list[::-1]\n",
    "digits_df['image'] = list(map(embeddable_image, patch_blocks_list))\n",
    "\n",
    "similarity_list_3.insert(0, max(similarity_list_3))\n",
    "digits_df['Color'] = similarity_list_3\n",
    "\n",
    "datasource = ColumnDataSource(digits_df)\n",
    "color_mapper = LinearColorMapper(palette=Inferno256, low=digits_df['Color'].min(), high=digits_df['Color'].max())\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of image',\n",
    "    width=1500,\n",
    "    height=1500,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>Position:</span>\n",
    "        <span style='font-size: 18px'>@Position</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.scatter(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color={'field': 'Color', 'transform': color_mapper},\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "plot_figure.scatter(\n",
    "    umap_embeddings[0,0],\n",
    "    umap_embeddings[0,1],\n",
    "    color='red',\n",
    "    size=15,\n",
    "    marker='o'\n",
    ")\n",
    "show(plot_figure)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
