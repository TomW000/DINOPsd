{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8eb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import get_threshold, resize_tiff_image, resize_hdf_image, compute_similarity_matrix, get_augmented_coordinates, display_tiff_image_grid, get_resized_volume, get_resized_ground_truth\n",
    "from setup import neurotransmitters, model_size, device, feat_dim, resize_size, curated_idx, few_shot_transforms, model, embeddings_path, save_path\n",
    "from setup import np, plt, torch, tqdm, sns, os, h5py, tifffile\n",
    "from perso_utils import get_fnames, load_image\n",
    "from DINOSim import DinoSim_pipeline\n",
    "from napari_dinosim.utils import get_img_processing_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )\n",
    "\n",
    "files, labels = zip(*get_fnames()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_factor = resize_size/130\n",
    "resize = lambda x: resize_factor*x\n",
    "\n",
    "emb_labels = np.hstack([[neuro]*feat_dim*600 for neuro in neurotransmitters]).reshape(-1) # FIXME: LOOKS WEIRD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc728a0",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Extracting reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['A','D','Ga','Glu','O','S']\n",
    "\n",
    "indices = [\n",
    "    [1,3,6,8,9],\n",
    "    [1,2,4,5,6],\n",
    "    [0,1,2,4,5],\n",
    "    [1,2,3,6,7],\n",
    "    [1,2,5,6,8],\n",
    "    [0,6,10,11,14]\n",
    "    ]\n",
    "\n",
    "coords = [\n",
    "    [(69,63.5),(68,61),(83,57),(76,62),(60,63)],\n",
    "    [(66,62),(58.5,64),(64,60),(62.5,65),(64,71)],\n",
    "    [(65,67),(72,60),(63,72),(60,67),(69,66.5)],\n",
    "    [(65,66),(64,71),(62,58.5),(62,68),(69,55)],\n",
    "    [(66,60),(60,70),(61,66.6),(58.5,63.5),(62.5,70.5)],\n",
    "    [(63,73),(58,69),(60,69),(66,64),(62,71)]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06e11c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Reference Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ref_embeddings(saved_ref_embeddings=False, \n",
    "                           embs_path=None, \n",
    "                           k=10,\n",
    "                           data_aug=False):\n",
    "\n",
    "    if saved_ref_embeddings:\n",
    "        \n",
    "        mean_ref = torch.load(embs_path, weights_only=False)\n",
    "        return mean_ref\n",
    "\n",
    "    else:\n",
    "\n",
    "        if data_aug:    \n",
    "            nb_transformations = len(few_shot_transforms)\n",
    "            \n",
    "            # Preload images and metadata once\n",
    "            good_images = []\n",
    "            transformed_coordinates = []\n",
    "\n",
    "            for idx in curated_idx:\n",
    "                img, coord_x, coord_y = load_image(files[idx])\n",
    "                good_images.append(img.transpose(1,2,0))\n",
    "                transformed_coordinates.append([(0, coord_x, coord_y)] * nb_transformations)\n",
    "\n",
    "            transformed_images = []\n",
    "            for image in good_images:\n",
    "                transformed = [t(image).permute(1,2,0) for t in few_shot_transforms]\n",
    "                transformed_images.extend(transformed)\n",
    "\n",
    "            for j, img in enumerate(transformed_images):\n",
    "                if img.shape != torch.Size([130, 130, 1]):\n",
    "                    h, w = img.shape[:2]\n",
    "                    h_diff = (130 - h) // 2\n",
    "                    w_diff = (130 - w) // 2\n",
    "                    padded_img = torch.zeros(130, 130, 1)\n",
    "                    padded_img[h_diff:h+h_diff, w_diff:w+w_diff, :] = img\n",
    "                    transformed_images[j] = padded_img\n",
    "                    \n",
    "            batch_size = int(len(curated_idx)/len(neurotransmitters)*nb_transformations) # nb of images in per class\n",
    "            good_datasets = [transformed_images[i:i+batch_size] for i in range(0,len(transformed_images),batch_size)]\n",
    "            good_datasets = np.array(good_datasets)\n",
    "            \n",
    "            transformed_coordinates = np.vstack(transformed_coordinates)\n",
    "            good_coordinates = [transformed_coordinates[i:i+batch_size] for i in range(0,len(transformed_coordinates),batch_size)]\n",
    "\n",
    "        else:\n",
    "            ref_embs_list = []\n",
    "            for i, index in tqdm(enumerate(indices)):\n",
    "                dataset_slice = files[i*600:(i+1)*600]\n",
    "                imgs = [resize_hdf_image(load_image(dataset_slice[k])[0]) for k in index]\n",
    "                coordinates = [list(map(resize, c)) for c in coords[i]]\n",
    "                dataset = list(zip(imgs, coordinates))\n",
    "                class_wise_embs_list = []\n",
    "                for image, reference in dataset:\n",
    "                    few_shot.pre_compute_embeddings(\n",
    "                        image[None,:,:,:],\n",
    "                        verbose=False,\n",
    "                        batch_size=1\n",
    "                    )\n",
    "                    few_shot.set_reference_vector(get_augmented_coordinates(reference), filter=None)\n",
    "                    closest_embds = few_shot.get_k_closest_elements(k=k, return_indices=False) # list of vectors\n",
    "                    class_wise_embs_list.append(torch.mean(closest_embds.cpu(), dim=0)) # list of lists of vectors\n",
    "                ref_embs_list.append(class_wise_embs_list) # list of lists of lists of vectors\n",
    "\n",
    "            ref_embs = np.array([np.mean(class_closest_embs, axis=0) for class_closest_embs in ref_embs_list])\n",
    "            \n",
    "            torch.save(ref_embs, os.path.join(embeddings_path, f'{model_size}_mean_ref_{resize_size}_Aug={data_aug}_k={k}'))\n",
    "            \n",
    "            return ref_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_refs = compute_ref_embeddings(True, '/home/tomwelch/Cambridge/Embeddings/giant_mean_ref_518_Aug=False_k=10')\n",
    "mean_refs = compute_ref_embeddings(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e9891",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Datasetwide Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e00fb",
   "metadata": {},
   "source": [
    "## w/o Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(saved_embeddings=False,\n",
    "                       embs_path=None):\n",
    "\n",
    "        if saved_embeddings:\n",
    "                dataset_embeddings = torch.load(embs_path)\n",
    "\n",
    "        else:\n",
    "                files, _ = zip(*get_fnames()) \n",
    "                dataset_embeddings = []\n",
    "                for file in files[:5]:\n",
    "                        with h5py.File(file) as f:\n",
    "                                volume = f['volumes/raw'][:][np.newaxis,:,:,:]\n",
    "                                volume_embeddings = []\n",
    "                                for z in range(volume.shape[-1]):\n",
    "                                        resized_slice = resize_hdf_image(volume[:,:,:,z])\n",
    "                                        few_shot.pre_compute_embeddings(\n",
    "                                                resized_slice,\n",
    "                                                batch_size=1\n",
    "                                                )\n",
    "                                        slice_embedding = few_shot.get_embeddings(reshape=False)\n",
    "                                        volume_embeddings.append(slice_embedding)\n",
    "                        dataset_embeddings.append(np.array(volume_embeddings))\n",
    "\n",
    "                dataset_embeddings = np.array(dataset_embeddings)\n",
    "                torch.save(dataset_embeddings, os.path.join(embeddings_path, f'{model_size}_dataset_embeddings_{resize_size}'))\n",
    "        \n",
    "        return dataset_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_embeddings = compute_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(embeddings: np.array, reference_embeddings, proportion):\n",
    "    \n",
    "    flattened_embeddings = embeddings.ravel()\n",
    "    assert emb_labels.shape == flattened_embeddings.shape\n",
    "    \n",
    "    similarity_list = []\n",
    "    for i, embedding in tqdm(enumerate(flattened_embeddings)):\n",
    "        neuro_class = emb_labels[i]\n",
    "        reference_index = neurotransmitters.index(neuro_class)\n",
    "        reference_vector = reference_embeddings[reference_index]\n",
    "        cosine_similarity = np.dot(reference_vector, embedding)/(np.linalg.norm(reference_vector)*np.linalg.norm(embedding))\n",
    "        similarity_list.append(cosine_similarity)\n",
    "    \n",
    "    similarity_tensor = np.array(similarity_list).reshape(embeddings.shape)\n",
    "    \n",
    "    pred_tensor = []\n",
    "    for volume in similarity_tensor:\n",
    "        threshold = get_threshold(volume, proportion)\n",
    "        pred = volume > threshold\n",
    "        pred = pred.astype(np.uint8)\n",
    "        pred_tensor.append(pred)\n",
    "    \n",
    "    pred_tensor\n",
    "    \n",
    "    for embedding in embeddings:\n",
    "        flattened_volume = embedding.reshape(-1)\n",
    "        similarity_list = [np.dot(reference, patch)/(np.linalg.norm(reference)*np.linalg.norm(patch)) for patch in flattened_volume]\n",
    "        similarity_matrix = np.array(similarity_list).reshape(embedding.shape)\n",
    "        threshold = get_threshold(similarity_matrix, proportion)\n",
    "        pred = similarity_matrix > threshold\n",
    "        pred = pred.astype(np.uint8)\n",
    "        pred_tensor.append(pred)\n",
    "    return pred_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b3090",
   "metadata": {},
   "source": [
    "## w/ Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(saved_embeddings=False,\n",
    "                    embs_path=None,\n",
    "                    reference_embeddings=None,\n",
    "                    proportion=0.99):\n",
    "\n",
    "        if saved_embeddings:\n",
    "                dataset_embeddings = torch.load(embs_path)\n",
    "\n",
    "        else:\n",
    "\n",
    "                files, _ = zip(*get_fnames()) \n",
    "                for n, file in enumerate(files):\n",
    "                        with h5py.File(file) as f:\n",
    "                                volume = f['volumes/raw'][:]\n",
    "                                '''\n",
    "                                volume_embeddings = []\n",
    "                                for z in tqdm(range(volume.shape[-1])):\n",
    "                                        resized_slice = resize_hdf_image(volume[None,:,:,z])\n",
    "                                        \n",
    "                                        few_shot.pre_compute_embeddings(\n",
    "                                                resized_slice[None,...],\n",
    "                                                batch_size=1,\n",
    "                                                verbose=False\n",
    "                                                )\n",
    "                                        slice_embedding = few_shot.get_embeddings(reshape=False)\n",
    "                                        volume_embeddings.append(slice_embedding)\n",
    "                                '''\n",
    "                                few_shot.pre_compute_embeddings(\n",
    "                                                volume[None,...].transpose(3,1,2,0),\n",
    "                                                batch_size=65,\n",
    "                                                verbose=True\n",
    "                                                )\n",
    "                                volume_embeddings = few_shot.get_embeddings(reshape=False)\n",
    "                                volume_embeddings = np.array(volume_embeddings.cpu())\n",
    "\n",
    "                                flattened_volume_embeddings = volume_embeddings.reshape(-1, feat_dim)\n",
    "                                path = os.path.normpath(file)\n",
    "                                neuro_class = path.split(os.sep)[-2] \n",
    "\n",
    "                                reference_index = neurotransmitters.index(neuro_class)\n",
    "                                reference_vector = reference_embeddings[reference_index]\n",
    "                                cosine_similarities = [np.dot(reference_vector, embedding)/(np.linalg.norm(reference_vector)*np.linalg.norm(embedding)) for embedding in flattened_volume_embeddings]\n",
    "                                similarity_volume = np.array(cosine_similarities).reshape(volume_embeddings.shape[:3])\n",
    "\n",
    "                                threshold = get_threshold(similarity_volume, proportion)\n",
    "                                pred = similarity_volume > threshold\n",
    "                                pred = pred.astype(np.uint8)\n",
    "                                \n",
    "                                number = n % 600\n",
    "                                \n",
    "                                torch.cuda.empty_cache()\n",
    "\n",
    "                                \n",
    "                                yield pred, volume, neuro_class, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db616715",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_generator = get_prediction(reference_embeddings=mean_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef08f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(len(files))):\n",
    "    prediction, volume, neuro_class, number = next(prediction_generator)\n",
    "    np.save(file=os.path.join(save_path, 'Predictions', f'{neuro_class}_prediction_{number:04d}.npy'), arr=prediction) # FIXME: Check axis \n",
    "    tifffile.imwrite(os.path.join(save_path, 'Volumes', f'{neuro_class}_volume_{number:04d}.tiff'), volume.transpose(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= np.load('/home/tomwelch/Cambridge/Output/Predictions/acetylcholine_prediction_0000.npy').transpose(0,2,1) # -> zxy to zyx\n",
    "pred = preds[75]\n",
    "from tifffile import imread\n",
    "gt = imread('/home/tomwelch/Cambridge/Output/Volumes/acetylcholine_volume_0000.tiff')[75]\n",
    "\n",
    "h, w = pred.shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5), dpi=150)\n",
    "\n",
    "# Show the background image\n",
    "ax.imshow(gt, cmap='gray', extent=[0, h, w, 0])\n",
    "\n",
    "# Overlay the heatmap\n",
    "sns.heatmap(\n",
    "    pred,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.5,             # Make the heatmap semi-transparent\n",
    "    ax=ax,\n",
    "    cbar=True,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False\n",
    ")\n",
    "\n",
    "plt.title(\"Patch Similarities\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.shape # yxz -> zyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d31f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "\n",
    "tifffile.imwrite('test.tif', volume.transpose(2,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4d33d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "img = imread('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test/x/frarobo_conn_18925456_images.tif')[6]\n",
    "resized_image = resize_tiff_image(img)\n",
    "\n",
    "resize_factor = np.multiply([resize_size, resize_size], 1/np.array(img.shape))\n",
    "coordinates = (resize_factor[0] * 153, resize_factor[1] * 131.5)\n",
    "aug_coordinates = get_augmented_coordinates(coordinates)\n",
    "\n",
    "few_shot.pre_compute_embeddings(\n",
    "    dataset=resized_image[None,:,:,:], \n",
    "    batch_size=1\n",
    "    )\n",
    "embeddings = few_shot.get_embeddings(reshape=False)\n",
    "\n",
    "few_shot.set_reference_vector(aug_coordinates)\n",
    "reference = few_shot.get_reference_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cbef22",
   "metadata": {},
   "source": [
    "# Generate ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test'\n",
    "x = sorted(os.listdir(os.path.join(path, 'x')), key=os.path.basename)\n",
    "y = sorted(os.listdir(os.path.join(path, 'y')), key=os.path.basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "test_volume = get_resized_volume('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test/x/frarobo_conn_18925456_images.tif')\n",
    "\n",
    "nb_slices = test_volume.shape[0]\n",
    "\n",
    "few_shot.pre_compute_embeddings(\n",
    "    dataset=test_volume, \n",
    "    batch_size=nb_slices\n",
    "    )\n",
    "new_embeddings = few_shot.get_embeddings(reshape=False)\n",
    "\n",
    "similarity_tensor = []\n",
    "for i in tqdm(range(nb_slices)):\n",
    "    similarity = compute_similarity_matrix(reference, new_embeddings[i])\n",
    "    similarity_tensor.append(similarity)\n",
    "    \n",
    "similarity_tensor = np.array(similarity_tensor)\n",
    "\n",
    "threshold = get_threshold(similarity_tensor[6], 0.95)\n",
    "pred = similarity_tensor > threshold\n",
    "pred = pred.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfe902",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = get_resized_ground_truth('/Users/tomw/Documents/MVA/Internship/Cambridge/Datasets/EM_Data_copy/original/test/y/frarobo_conn_18925456_masks.tif').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "score = jaccard_score(gt.ravel(), pred.ravel())\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
