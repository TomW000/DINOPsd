{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import neurotransmitters, model, model_size, device, feat_dim, resize_size, dataset_path, curated_idx, few_shot_transforms\n",
    "from setup import tqdm, torch, np, os, h5py, sns, plt, tqdm, Trans, Image, chain\n",
    "from setup import cosine_similarity, euclidean_distances\n",
    "from perso_utils import get_fnames, load_image, get_latents\n",
    "from DINOSim import DinoSim_pipeline, diplay_features\n",
    "from napari_dinosim.utils import get_img_processing_f\n",
    "\n",
    "# Create an instance of the pipeline (not just assigning the class)\n",
    "few_shot = DinoSim_pipeline(model,\n",
    "                            model.patch_size,\n",
    "                            device,\n",
    "                            get_img_processing_f(resize_size),\n",
    "                            feat_dim, \n",
    "                            dino_image_size=resize_size\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "data_aug = True\n",
    "\n",
    "files, labels = zip(*get_fnames()) \n",
    "\n",
    "\n",
    "if data_aug:    \n",
    "    nb_transformations = len(few_shot_transforms)\n",
    "    nb_trans_images = nb_transformations * len(curated_idx)\n",
    "\n",
    "    good_datasets = []\n",
    "    good_images = np.vstack([[load_image(files[idx])[0]]*nb_transformations for idx in curated_idx]).transpose(0,2,3,1)\n",
    "    good_labels = np.hstack([[labels[idx]]*nb_transformations for idx in curated_idx])\n",
    "    coordinates = np.vstack([[(0, load_image(files[idx])[1], load_image(files[idx])[2])]*nb_transformations for idx in curated_idx])\n",
    "    batch = []\n",
    "    for k in range(0, nb_trans_images, nb_transformations):\n",
    "        for image in good_images[k:k+nb_transformations]:\n",
    "            batch.append([few_shot_transforms[j](image) for j in range(nb_transformations)])\n",
    "            if len(batch) == 10:\n",
    "                good_datasets.append(list(chain.from_iterable(batch)))\n",
    "                batch = []        \n",
    "\n",
    "else:\n",
    "    good_datasets = np.array([load_image(files[idx])[0] for idx in curated_idx]).transpose(0,2,3,1)\n",
    "    good_labels = np.array([labels[idx] for idx in curated_idx])\n",
    "    good_coordinates = np.array([(0, load_image(files[idx])[1], load_image(files[idx])[2]) for idx in curated_idx])\n",
    "\n",
    "\n",
    "latent_list, label_list = [], []\n",
    "for dataset, batch_label in tqdm(zip(good_datasets, good_labels), desc='Iterating through neurotransmitters'):\n",
    "\n",
    "    # Pre-compute embeddings\n",
    "    few_shot.pre_compute_embeddings(\n",
    "        dataset,  # Pass numpy array of images\n",
    "        overlap=(0.5, 0.5),\n",
    "        padding=(0, 0),\n",
    "        crop_shape=(140, 140, 1),\n",
    "        verbose=True,\n",
    "        batch_size=10\n",
    "    )\n",
    "    \n",
    "    # Set reference vectors\n",
    "    few_shot.set_reference_vector(good_coordinates, filter=None)\n",
    "    ref = few_shot.get_refs()\n",
    "    \n",
    "    # Get closest elements - using the correct method name\n",
    "    \n",
    "    close_embedding =  few_shot.get_k_closest_elements(k=k)\n",
    "    #k_labels = [l for _ in range(k)]\n",
    "    k_labels =  [batch_label[0] for _ in range(k)]\n",
    "\n",
    "    \n",
    "    # Convert to numpy for storing\n",
    "    close_embedding_np = close_embedding.cpu().numpy() if isinstance(close_embedding, torch.Tensor) else close_embedding\n",
    "    \n",
    "    latent_list.append(close_embedding_np)\n",
    "    label_list.append(k_labels)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    few_shot.delete_precomputed_embeddings()\n",
    "    few_shot.delete_references()\n",
    "    \n",
    "mean_ref = torch.from_numpy(np.vstack([np.mean(list, axis=0) for list in latent_list]))\n",
    "# Stack all embeddings and labels\n",
    "ref_latents = np.vstack(latent_list)\n",
    "ref_labels = np.hstack(label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
